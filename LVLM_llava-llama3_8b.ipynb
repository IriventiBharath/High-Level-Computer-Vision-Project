{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "328557ec",
   "metadata": {},
   "source": [
    "SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8837060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLaVA-Llama3 Vision Language Model Setup and Evaluation\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "import csv\n",
    "import re\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from langchain_ollama import OllamaLLM\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b49560",
   "metadata": {},
   "source": [
    "DOCVQA DATASET (RUN ONLY THE CONFIGURATION YOU WANT TO ASSESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8c0afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "os.environ[\"OLLAMA_NUM_GPU_LAYERS\"] = \"20\"\n",
    "data_dir = \"docvqa_samples_300\"\n",
    "image_dir = os.path.join(data_dir, \"images\")\n",
    "metadata_file = os.path.join(data_dir, \"metadata.json\")\n",
    "output_csv = \"results/vlm_results.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef1d00c",
   "metadata": {},
   "source": [
    "NEW DATASET (RUN ONLY THE CONFIGURATION YOU WANT TO ASSESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73de7365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "os.environ[\"OLLAMA_NUM_GPU_LAYERS\"] = \"20\"\n",
    "data_dir = \"NewDataset\"\n",
    "image_dir = os.path.join(data_dir, \"images\")\n",
    "metadata_file = os.path.join(data_dir, \"metadata.json\")\n",
    "output_csv = \"results_NEWDATA/vlm_results_NEWDATASET.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9096ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ollama connection successful with llava-llama3\n",
      "Processing sample 1/10\n",
      "Processing sample 2/10\n",
      "Processing sample 3/10\n",
      "Processing sample 4/10\n",
      "Processing sample 5/10\n",
      "Processing sample 6/10\n",
      "Processing sample 7/10\n",
      "Processing sample 8/10\n",
      "Processing sample 9/10\n",
      "Processing sample 10/10\n",
      "Evaluation Summary on 10 samples:\n",
      "Avg Exact Match: 20.00%\n",
      "Avg F1 Score: 29.23%\n",
      "Results saved to: vlm_results_NEWDATASET.csv\n"
     ]
    }
   ],
   "source": [
    "# Load metadata\n",
    "with open(metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Evaluation functions\n",
    "def preprocess_answer(text):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def exact_match(pred, ground_truths):\n",
    "    pred_processed = preprocess_answer(pred)\n",
    "    return any(pred_processed == preprocess_answer(gt) for gt in ground_truths)\n",
    "\n",
    "def f1(pred, ground_truths):\n",
    "    def score(pred, gt):\n",
    "        pred_tokens = preprocess_answer(pred).split()\n",
    "        gt_tokens = preprocess_answer(gt).split()\n",
    "        common = set(pred_tokens) & set(gt_tokens)\n",
    "        if not common:\n",
    "            return 0.0\n",
    "        precision = len(common) / len(pred_tokens) if pred_tokens else 0.0\n",
    "        recall = len(common) / len(gt_tokens) if gt_tokens else 0.0\n",
    "        return 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    return max(score(pred, gt) for gt in ground_truths)\n",
    "\n",
    "def pil_to_base64(pil_img):\n",
    "    buffered = BytesIO()\n",
    "    pil_img.save(buffered, format=\"PNG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# LLM Setup\n",
    "try:\n",
    "    llm = OllamaLLM(model=\"llava-llama3\")\n",
    "    test_response = llm.invoke(\"Hello\")\n",
    "    print(\"✅ Ollama connection successful with llava-llama3\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error connecting to Ollama: {e}\")\n",
    "\n",
    "# Main processing pipeline\n",
    "with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    fieldnames = [\"id\", \"image_filename\", \"question\", \"ground_truth\", \"predicted_answer\", \"exact_match\", \"f1_score\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    em_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for i, sample in enumerate(metadata):\n",
    "        print(f\"Processing sample {i+1}/{len(metadata)}\")\n",
    "        \n",
    "        idx = sample[\"id\"]\n",
    "        img_path = os.path.join(image_dir, sample[\"image_filename\"])\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"⚠️ Image not found: {img_path}\")\n",
    "            continue\n",
    "            \n",
    "        image = Image.open(img_path)\n",
    "        question = sample[\"question\"]\n",
    "        ground_truths = sample[\"answers\"]\n",
    "\n",
    "        # Convert image to base64\n",
    "        image_b64 = pil_to_base64(image)\n",
    "\n",
    "        # Create prompt for vision model\n",
    "        prompt = f\"Question: {question}\\n\\nAnswer this question based on what you see in the image. Provide only the specific answer requested, be concise.\"\n",
    "\n",
    "        try:\n",
    "            response = llm.invoke(prompt, images=[image_b64])\n",
    "            pred_answer = str(response).strip()\n",
    "        except Exception as e:\n",
    "            pred_answer = \"\"\n",
    "            print(f\"⚠️ Error processing {sample['image_filename']}: {e}\")\n",
    "\n",
    "        em = exact_match(pred_answer, ground_truths)\n",
    "        f1_val = f1(pred_answer, ground_truths)\n",
    "\n",
    "        em_scores.append(int(em))\n",
    "        f1_scores.append(f1_val)\n",
    "\n",
    "        writer.writerow({\n",
    "            \"id\": idx,\n",
    "            \"image_filename\": sample[\"image_filename\"],\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": \" | \".join(ground_truths),\n",
    "            \"predicted_answer\": pred_answer,\n",
    "            \"exact_match\": em,\n",
    "            \"f1_score\": round(f1_val, 2)\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b73bdbe",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e4c2dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.292\n",
      "Average Exact Match (EM) Score: 0.200\n"
     ]
    }
   ],
   "source": [
    "# Load results from OUTPUT_CSV and compute average F1 and EM scores\n",
    "results_df = pd.read_csv(output_csv)\n",
    "avg_f1 = results_df['f1_score'].mean()\n",
    "avg_em = results_df['exact_match'].mean()\n",
    "\n",
    "print(f\"Average F1 Score: {avg_f1:.3f}\")\n",
    "print(f\"Average Exact Match (EM) Score: {avg_em:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
