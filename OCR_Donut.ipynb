{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Donut DocVQA Implementation\n",
    "\n",
    "This notebook implements Donut for Document Visual Question Answering (DocVQA) using the fine-tuned model.\n",
    "\n",
    "## Features:\n",
    "- Uses `donut-base-finetuned-docvqa` model\n",
    "- Direct VQA approach: Image + Question â†’ Answer\n",
    "- Proper error handling and evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import pipeline, DonutProcessor, VisionEncoderDecoderModel\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOCVQA DATASET (RUN ONLY THE CONFIGURATION YOU WANT TO ASSESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATA_DIR = 'docvqa_samples_300'\n",
    "IMAGE_DIR = os.path.join(DATA_DIR, \"images\")\n",
    "METADATA_FILE = os.path.join(DATA_DIR, \"metadata.json\")\n",
    "OUTPUT_CSV = \"results\\Donut_finetuned_results.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW DATASET (RUN ONLY THE CONFIGURATION YOU WANT TO ASSESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = \"NewDataset\"\n",
    "IMAGE_DIR = os.path.join(DATA_DIR, \"images\")\n",
    "METADATA_FILE = os.path.join(DATA_DIR, \"metadata.json\")\n",
    "OUTPUT_CSV = \"results_NEWDATA/OCR_DONUT_RESULTS_NEWDATASET.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"naver-clova-ix/donut-base-finetuned-docvqa\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 1\n",
    "CLEAR_CACHE_EVERY = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \"\"\"Normalize text for evaluation\"\"\"\n",
    "    return text.lower().translate(str.maketrans('', '', string.punctuation)).strip()\n",
    "\n",
    "def exact_match(pred, ground_truths):\n",
    "    \"\"\"Calculate exact match score\"\"\"\n",
    "    pred_norm = normalize(pred)\n",
    "    return any(pred_norm == normalize(gt) for gt in ground_truths)\n",
    "\n",
    "def f1_score(pred, ground_truths):\n",
    "    \"\"\"Calculate F1 score between prediction and ground truth answers\"\"\"\n",
    "    def score(pred, gt):\n",
    "        pred_tokens = normalize(pred).split()\n",
    "        gt_tokens = normalize(gt).split()\n",
    "        common = set(pred_tokens) & set(gt_tokens)\n",
    "        \n",
    "        if not common:\n",
    "            return 0.0\n",
    "            \n",
    "        precision = len(common) / len(pred_tokens) if pred_tokens else 0.0\n",
    "        recall = len(common) / len(gt_tokens) if gt_tokens else 0.0\n",
    "        \n",
    "        return 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return max(score(pred, gt) for gt in ground_truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Donut DocVQA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DonutDocVQA:\n",
    "    \"\"\"Donut DocVQA model implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=MODEL_NAME):\n",
    "        try:\n",
    "            self.pipeline = pipeline(\n",
    "                task=\"document-question-answering\",\n",
    "                model=model_name,\n",
    "                device=0 if torch.cuda.is_available() else -1,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "            )\n",
    "            self.method = \"pipeline\"\n",
    "        except Exception as e:\n",
    "            self.processor = DonutProcessor.from_pretrained(model_name)\n",
    "            self.model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "            self.model.to(device)\n",
    "            self.model.eval()\n",
    "            self.method = \"manual\"\n",
    "    \n",
    "    def answer_question(self, image_path, question):\n",
    "        \"\"\"Answer a question about a document image\"\"\"\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            \n",
    "            if self.method == \"pipeline\":\n",
    "                result = self.pipeline(image=image, question=question)\n",
    "                return result[0]['answer'] if result else \"\"\n",
    "            else:\n",
    "                task_prompt = f\"<s_docvqa><s_question>{question}</s_question><s_answer>\"\n",
    "                \n",
    "                pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "                pixel_values = pixel_values.to(device)\n",
    "                \n",
    "                decoder_input_ids = self.processor.tokenizer(\n",
    "                    task_prompt, \n",
    "                    add_special_tokens=False,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).input_ids\n",
    "                decoder_input_ids = decoder_input_ids.to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model.generate(\n",
    "                        pixel_values,\n",
    "                        decoder_input_ids=decoder_input_ids,\n",
    "                        max_length=self.model.decoder.config.max_position_embeddings,\n",
    "                        pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "                        eos_token_id=self.processor.tokenizer.eos_token_id,\n",
    "                        use_cache=True,\n",
    "                        bad_words_ids=[[self.processor.tokenizer.unk_token_id]],\n",
    "                        return_dict_in_generate=True,\n",
    "                        do_sample=False,\n",
    "                        num_beams=1,\n",
    "                    )\n",
    "                \n",
    "                sequence = self.processor.batch_decode(outputs.sequences)[0]\n",
    "                sequence = sequence.replace(self.processor.tokenizer.eos_token, \"\").replace(self.processor.tokenizer.pad_token, \"\")\n",
    "                \n",
    "                answer_start = sequence.find(\"<s_answer>\") + len(\"<s_answer>\")\n",
    "                answer_end = sequence.find(\"</s_answer>\")\n",
    "                \n",
    "                if answer_start > len(\"<s_answer>\") - 1 and answer_end > answer_start:\n",
    "                    answer = sequence[answer_start:answer_end].strip()\n",
    "                else:\n",
    "                    answer = sequence.replace(task_prompt, \"\").strip()\n",
    "                \n",
    "                return answer\n",
    "        except Exception as e:\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_with_donut_docvqa(sample, image_dir, donut_vqa):\n",
    "    \"\"\"Process a single document using Donut DocVQA\"\"\"\n",
    "    try:\n",
    "        doc_id = sample['id']\n",
    "        image_filename = sample['image_filename']\n",
    "        question = sample['question']\n",
    "        ground_truth = sample['answers']\n",
    "\n",
    "        image_path = os.path.join(image_dir, image_filename)\n",
    "        \n",
    "        # Get answer directly from Donut DocVQA\n",
    "        predicted_answer = donut_vqa.answer_question(image_path, question)\n",
    "        \n",
    "        if not predicted_answer:\n",
    "            return None\n",
    "\n",
    "        # Evaluate the prediction\n",
    "        em = exact_match(predicted_answer, ground_truth)\n",
    "        f1_val = f1_score(predicted_answer, ground_truth)\n",
    "\n",
    "        return {\n",
    "            \"id\": doc_id,\n",
    "            \"image_filename\": image_filename,\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": \" | \".join(ground_truth),\n",
    "            \"extracted_content\":  predicted_answer,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"exact_match\": em,\n",
    "            \"f1_score\": round(f1_val, 2)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load metadata\n",
    "with open(METADATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    docvqa_metadata = json.load(f)\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# Initialize Donut DocVQA model\n",
    "donut_vqa = DonutDocVQA(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   0%|          | 0/10 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "`sdpa` attention does not support `output_attentions=True` or `head_mask`. Please set your attention to `eager` if you want any of these features.\n",
      "Processing documents: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:10<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "# Process all documents\n",
    "all_results = []\n",
    "processed_count = 0\n",
    "\n",
    "with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    fieldnames = [\"id\", \"image_filename\", \"question\", \"ground_truth\", \n",
    "                  \"extracted_content\", \"predicted_answer\", \"exact_match\", \"f1_score\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for i, sample in enumerate(tqdm(docvqa_metadata, desc=\"Processing documents\")):\n",
    "        try:\n",
    "            result = process_document_with_donut_docvqa(sample, IMAGE_DIR, donut_vqa)\n",
    "            \n",
    "            if result is not None:\n",
    "                writer.writerow(result)\n",
    "                all_results.append(result)\n",
    "                processed_count += 1\n",
    "            \n",
    "            # Memory management\n",
    "            if (i + 1) % CLEAR_CACHE_EVERY == 0:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.100\n",
      "Average Exact Match (EM) Score: 0.100\n"
     ]
    }
   ],
   "source": [
    "# Load results from OUTPUT_CSV and compute average F1 and EM scores\n",
    "results_df = pd.read_csv(OUTPUT_CSV)\n",
    "avg_f1 = results_df['f1_score'].mean()\n",
    "avg_em = results_df['exact_match'].mean()\n",
    "\n",
    "print(f\"Average F1 Score: {avg_f1:.3f}\")\n",
    "print(f\"Average Exact Match (EM) Score: {avg_em:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
